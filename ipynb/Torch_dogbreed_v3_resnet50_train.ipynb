{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.5.1\n",
      "Torchvision Version:  0.6.1\n"
     ]
    }
   ],
   "source": [
    "# From: https://www.kaggle.com/c/dog-breed-identification/data\n",
    "# Author: Morpheus Hsieh (morpheus.hsieh@gmail.com)\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import os, sys\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from os import listdir\n",
    "from os.path import join, exists, isfile\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "{\n",
      "  'DataPath'    : 'D:\\GitWork\\dog_breed\\data',\n",
      "  'OutPath'     : 'D:\\GitWork\\dog_breed\\output',\n",
      "  'ProcPath'    : 'D:\\GitWork\\dog_breed\\processed',\n",
      "  'PreTrainPath': 'D:\\GitWork\\dog_breed\\pretrained',\n",
      "  'PreTrainFile': '',\n",
      "  'TestPath'    : 'D:\\Dataset\\dog-breed-identification\\test',\n",
      "  'TrainPath'   : 'D:\\Dataset\\dog-breed-identification\\train',\n",
      "  'CsvLabel'    : 'labels.csv',\n",
      "  'BatchSize'   : 16,\n",
      "  'FracForTrain': 0.8\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "Params = {\n",
    "    'DataPath'    : r'D:\\GitWork\\dog_breed\\data',\n",
    "    'OutPath'     : r'D:\\GitWork\\dog_breed\\output',\n",
    "    'ProcPath'    : r'D:\\GitWork\\dog_breed\\processed',\n",
    "    'PreTrainPath': r'D:\\GitWork\\dog_breed\\pretrained',\n",
    "    'PreTrainFile': '',\n",
    "    'TestPath'    : r'D:\\Dataset\\dog-breed-identification\\test',\n",
    "    'TrainPath'   : r'D:\\Dataset\\dog-breed-identification\\train',\n",
    "    'CsvLabel'    : 'labels.csv',\n",
    "    'BatchSize'   : 16,\n",
    "    'FracForTrain': 0.8\n",
    "}\n",
    "\n",
    "\n",
    "def prettyDict(dic, indent=2):\n",
    "    array = []\n",
    "    key_maxlen = 0\n",
    "    item_cnt = 0\n",
    "    item_size = len(dic)\n",
    "    split_str = ': '\n",
    "    \n",
    "    for key, val in dic.items():\n",
    "        if key_maxlen < len(str(key)): \n",
    "            key_maxlen = len(str(key))\n",
    "        \n",
    "        tmpstr = ''\n",
    "        tmpstr += f\"'{key}'\" if isinstance(key, str) else f\"{key}\"\n",
    "        tmpstr += split_str\n",
    "        tmpstr += f\"'{val}'\" if isinstance(val, str) else f\"{val}\"\n",
    "\n",
    "        item_cnt += 1\n",
    "        if item_cnt < item_size: tmpstr += ','\n",
    "        array.append(tmpstr)\n",
    "    \n",
    "    for i in range(len(array)):\n",
    "        inStr = array[i]\n",
    "        ary = inStr.split(split_str)\n",
    "        key = ary[0].ljust(key_maxlen+2)\n",
    "        val = ary[1]\n",
    "        array[i] = (' '*indent) + key + ': ' + val\n",
    "        \n",
    "    outstr = '{\\n' + '\\n'.join(array) + '\\n}'\n",
    "    return outstr\n",
    "\n",
    "outstr = prettyDict(Params)\n",
    "print('Parameters:')\n",
    "print(outstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10222 entries, 0 to 10221\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      10222 non-null  object\n",
      " 1   breed   10222 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 159.8+ KB\n",
      "None\n",
      "\n",
      "                                 id             breed\n",
      "0  000bec180eb18c7604dcecc8fe0dba07       boston_bull\n",
      "1  001513dfcb2ffafc82cccf4d8bbaba97             dingo\n",
      "2  001cdf01b096e06d78e9e5112d419397          pekinese\n",
      "3  00214f311d5d2247d5dfe4fe24b2303d          bluetick\n",
      "4  0021f9ceb3235effd7fcde7f7538ed62  golden_retriever\n"
     ]
    }
   ],
   "source": [
    "# Read breed information from csv\n",
    "DataPath = Params.get('DataPath')\n",
    "csv_labels = Params.get('CsvLabel')\n",
    "f_abspath = join(DataPath, csv_labels)\n",
    "\n",
    "df_labels = pd.read_csv(f_abspath)\n",
    "\n",
    "print(df_labels.info())\n",
    "print()\n",
    "print(df_labels.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120 entries, 0 to 119\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   breed_id  120 non-null    int64 \n",
      " 1   breed     120 non-null    object\n",
      " 2   count     120 non-null    int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 2.9+ KB\n",
      "None\n",
      "\n",
      "   breed_id                 breed  count\n",
      "0         0    scottish_deerhound    126\n",
      "1         1           maltese_dog    117\n",
      "2         2          afghan_hound    116\n",
      "3         3           entlebucher    115\n",
      "4         4  bernese_mountain_dog    114\n",
      "\n",
      "Num classes: 120\n"
     ]
    }
   ],
   "source": [
    "# Count all breeds\n",
    "def countBreeds(df):\n",
    "    df1 = df_labels.groupby(\"breed\")[\"id\"].count().reset_index(name=\"count\")\n",
    "    df1 = df1.sort_values(by='count', ascending=False).reset_index(drop=True)\n",
    "    df1.insert(0, 'breed_id', df1.index)\n",
    "    return df1\n",
    "\n",
    "df_breeds = countBreeds(df_labels)\n",
    "print(df_breeds.info())\n",
    "print()\n",
    "print(df_breeds.head())\n",
    "\n",
    "NumClasses = int(df_breeds.shape[0])\n",
    "print('\\nNum classes:', NumClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10222 entries, 0 to 10221\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   image     10222 non-null  object\n",
      " 1   breed_id  10222 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 159.8+ KB\n",
      "None\n",
      "                                               image  breed_id\n",
      "0  D:\\Dataset\\dog-breed-identification\\train\\000b...        42\n",
      "1  D:\\Dataset\\dog-breed-identification\\train\\0015...        72\n",
      "2  D:\\Dataset\\dog-breed-identification\\train\\001c...        94\n",
      "3  D:\\Dataset\\dog-breed-identification\\train\\0021...        50\n",
      "4  D:\\Dataset\\dog-breed-identification\\train\\0021...       115\n"
     ]
    }
   ],
   "source": [
    "# Process labels\n",
    "\n",
    "dict_bid_fw = dict(df_breeds[['breed', 'breed_id']].values)\n",
    "# print(dict_bid_fw)\n",
    "\n",
    "dict_bid_bw = dict(df_breeds[['breed_id', 'breed']].values)\n",
    "# print(dict_bid_bw)\n",
    "\n",
    "# Build processed labels file\n",
    "df_data = pd.DataFrame(columns=['image', 'breed_id'])\n",
    "df_data['breed_id'] = df_labels.breed.map(dict_bid_fw)\n",
    "\n",
    "TranPath = Params['TrainPath']\n",
    "\n",
    "df_data['image'] = df_labels.apply (\n",
    "    lambda row: join(TranPath, row['id']+'.jpg') \\\n",
    "    if exists(join(TranPath, row['id']+'.jpg')) else None, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(df_data.info())\n",
    "print(df_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataSet size: {'train': 8177, 'valid': 2045}\n",
      "\n",
      "Image shape: torch.Size([16, 3, 224, 224])\n",
      "Label shape: torch.Size([16])\n",
      "\n",
      "Image iid:\n",
      "  896ae4eda6310c5fa223131ac7574494\n",
      "  adc5daa413e246287aacf9b7f6e16d36\n",
      "  9c8bab83db2da5aad21f41e44643de64\n",
      "  5f11e117f4ba6ed026682fe516651c59\n",
      "  dd52583a6a9bfdcc5278c5d61a57b7e1\n",
      "  c9be1b14a664fab3d09d0ca4b8a79cbf\n",
      "  8befc822b56b744d72872428a0ef4851\n",
      "  278739637271fe972aa9a62404685a24\n",
      "  e4b586f1a120bba42545d866d45d0602\n",
      "  dfc97c3d7a59ad4d89e791e6ab14c49f\n",
      "  78e7aa41b7f4326a7f582426a3fdb151\n",
      "  c3487bbf17be7def166026f875edbdc0\n",
      "  bbb3573b1f9a2f3ab1b1d62cc26a7b88\n",
      "  c2a5c44a3cb6d8e5fc36ff0bb8640776\n",
      "  d5b01922a98bd60cd867e7b6d62039f2\n",
      "  5df814fe3a8b8b678baf9afe62998291\n",
      "\n",
      "Image shape: torch.Size([3, 224, 224])\n",
      "\n",
      "tensor([[[-0.2513, -0.1657, -0.6452,  ..., -1.5528, -1.2959, -0.5424],\n",
      "         [-0.9877, -0.6794, -0.5082,  ..., -1.2274, -0.8335, -0.2171],\n",
      "         [-1.0733, -1.4500, -0.4911,  ..., -0.8849, -0.4226, -0.0801],\n",
      "         ...,\n",
      "         [ 0.8104,  0.5707,  0.7248,  ..., -0.5938,  0.3994,  0.5536],\n",
      "         [ 0.8961,  0.5707,  0.5536,  ...,  0.5364,  0.6392,  0.7591],\n",
      "         [ 0.6563,  0.5707,  0.5022,  ...,  0.7419,  0.9646,  1.0502]],\n",
      "\n",
      "        [[-0.4076, -0.3200, -0.8102,  ..., -1.5980, -1.3004, -0.5126],\n",
      "         [-1.1253, -0.7927, -0.6352,  ..., -1.2654, -0.8277, -0.1975],\n",
      "         [-1.1078, -1.4930, -0.5476,  ..., -0.8978, -0.4251, -0.0574],\n",
      "         ...,\n",
      "         [ 0.7129,  0.4678,  0.6254,  ..., -0.6176,  0.3627,  0.5203],\n",
      "         [ 0.8004,  0.4678,  0.4503,  ...,  0.4503,  0.5553,  0.6604],\n",
      "         [ 0.5553,  0.4678,  0.3978,  ...,  0.6078,  0.8004,  0.8880]],\n",
      "\n",
      "        [[-0.2881, -0.2184, -0.7587,  ..., -1.5081, -1.3687, -0.6367],\n",
      "         [-0.9504, -0.6715, -0.5495,  ..., -1.1944, -0.9330, -0.3578],\n",
      "         [-0.9330, -1.3164, -0.4275,  ..., -0.8807, -0.5844, -0.2881],\n",
      "         ...,\n",
      "         [ 0.7402,  0.4962,  0.6879,  ..., -0.5670,  0.1825,  0.2696],\n",
      "         [ 0.8274,  0.5136,  0.5136,  ...,  0.4439,  0.3916,  0.4265],\n",
      "         [ 0.5834,  0.4962,  0.4614,  ...,  0.5136,  0.6356,  0.6879]]])\n",
      "\n",
      "Labels: tensor(34)\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean = [0.485, 0.456, 0.406],\n",
    "        std  = [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "class myDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, phase='train', frac=0.8, transform=None):\n",
    "        \n",
    "        num_rows = df.shape[0]\n",
    "        train_len = int(float(frac) * float(num_rows))\n",
    "        valid_len = num_rows - train_len\n",
    "        \n",
    "        # data = df.head(train_len) if phase=='train' else df.tail(valid_len)\n",
    "        \n",
    "        # get random image id without duplicates\n",
    "        idx_ary = np.arange(len(df))\n",
    "        idx_ary_rand = np.random.permutation(idx_ary) # random shuffle\n",
    "        \n",
    "        data_len = train_len if phase=='train' else valid_len\n",
    "        data = df.loc[idx_ary_rand[:data_len]]\n",
    "        \n",
    "        self.images = data['image'].tolist()\n",
    "        self.labels = data['breed_id'].tolist()\n",
    "\n",
    "        self.transform = transform\n",
    "        self.len = len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        f_abspath = self.images[index]\n",
    "        img_pil = Image.open(f_abspath)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img_pil)\n",
    "\n",
    "        lbl = int(self.labels[index])\n",
    "        iid = os.path.split(f_abspath)[1].replace('.jpg', '')\n",
    "        \n",
    "        return [img, lbl, iid]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "\n",
    "frac = Params['FracForTrain']\n",
    "\n",
    "phases = ['train', 'valid']\n",
    "dataSet = { \n",
    "    x: myDataset(df_data, phase=x, frac=frac, transform=transform) for x in phases \n",
    "}\n",
    "\n",
    "BatchSize = Params['BatchSize']\n",
    "dataLoader = {\n",
    "    x: DataLoader(dataSet[x], batch_size=BatchSize, shuffle=True) for x in phases\n",
    "}\n",
    "\n",
    "dataSizes = { x: len(dataSet[x]) for x in phases }\n",
    "print('DataSet size:', dataSizes)\n",
    "\n",
    "trainLoader = dataLoader['train']\n",
    "imgs, lbls, iids = next(iter(trainLoader))\n",
    "print('\\nImage shape:', imgs.size())\n",
    "print('Label shape:', lbls.size())\n",
    "\n",
    "print('\\nImage iid:')\n",
    "id_list = [''.join(iid) for iid in iids]\n",
    "print('  '+'\\n  '.join(id_list))\n",
    "\n",
    "img = imgs[0]\n",
    "print('\\nImage shape:', img.shape)\n",
    "print(); print(img)\n",
    "\n",
    "print('\\nLabels:', lbls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Use GPU for train\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=120, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Build Model\n",
    "def buildModel(use_gpu, numClasses, preTrainModel=None):\n",
    "    model = models.resnet50(pretrained=True)\n",
    "\n",
    "    # freeze all model parameters\n",
    "    for param in model.parameters():\n",
    "        model.requires_grad = False\n",
    "\n",
    "    # new final layer with 16 classes\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, numClasses)\n",
    "    \n",
    "    if preTrainModel is not None:\n",
    "        model.load_state_dict(preTrainModel)\n",
    "        \n",
    "    if use_gpu: \n",
    "        model = model.cuda()\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "TrainPath = Params['PreTrainPath']\n",
    "TrainFile = Params['PreTrainFile']\n",
    "f_pretrain = join(TrainPath, TrainFile)\n",
    "\n",
    "pretrain_model = None\n",
    "if exists(f_pretrain) and isfile(f_pretrain):\n",
    "    pretrain_model = torch.load(join(path, fname))\n",
    "\n",
    "model = buildModel(use_gpu, NumClasses, preTrainModel=pretrain_model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([64, 3, 7, 7])\n",
      "bn1.weight \t torch.Size([64])\n",
      "bn1.bias \t torch.Size([64])\n",
      "bn1.running_mean \t torch.Size([64])\n",
      "bn1.running_var \t torch.Size([64])\n",
      "bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.conv1.weight \t torch.Size([64, 64, 1, 1])\n",
      "layer1.0.bn1.weight \t torch.Size([64])\n",
      "layer1.0.bn1.bias \t torch.Size([64])\n",
      "layer1.0.bn1.running_mean \t torch.Size([64])\n",
      "layer1.0.bn1.running_var \t torch.Size([64])\n",
      "layer1.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn2.weight \t torch.Size([64])\n",
      "layer1.0.bn2.bias \t torch.Size([64])\n",
      "layer1.0.bn2.running_mean \t torch.Size([64])\n",
      "layer1.0.bn2.running_var \t torch.Size([64])\n",
      "layer1.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.conv3.weight \t torch.Size([256, 64, 1, 1])\n",
      "layer1.0.bn3.weight \t torch.Size([256])\n",
      "layer1.0.bn3.bias \t torch.Size([256])\n",
      "layer1.0.bn3.running_mean \t torch.Size([256])\n",
      "layer1.0.bn3.running_var \t torch.Size([256])\n",
      "layer1.0.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.downsample.0.weight \t torch.Size([256, 64, 1, 1])\n",
      "layer1.0.downsample.1.weight \t torch.Size([256])\n",
      "layer1.0.downsample.1.bias \t torch.Size([256])\n",
      "layer1.0.downsample.1.running_mean \t torch.Size([256])\n",
      "layer1.0.downsample.1.running_var \t torch.Size([256])\n",
      "layer1.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer1.1.conv1.weight \t torch.Size([64, 256, 1, 1])\n",
      "layer1.1.bn1.weight \t torch.Size([64])\n",
      "layer1.1.bn1.bias \t torch.Size([64])\n",
      "layer1.1.bn1.running_mean \t torch.Size([64])\n",
      "layer1.1.bn1.running_var \t torch.Size([64])\n",
      "layer1.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn2.weight \t torch.Size([64])\n",
      "layer1.1.bn2.bias \t torch.Size([64])\n",
      "layer1.1.bn2.running_mean \t torch.Size([64])\n",
      "layer1.1.bn2.running_var \t torch.Size([64])\n",
      "layer1.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer1.1.conv3.weight \t torch.Size([256, 64, 1, 1])\n",
      "layer1.1.bn3.weight \t torch.Size([256])\n",
      "layer1.1.bn3.bias \t torch.Size([256])\n",
      "layer1.1.bn3.running_mean \t torch.Size([256])\n",
      "layer1.1.bn3.running_var \t torch.Size([256])\n",
      "layer1.1.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer1.2.conv1.weight \t torch.Size([64, 256, 1, 1])\n",
      "layer1.2.bn1.weight \t torch.Size([64])\n",
      "layer1.2.bn1.bias \t torch.Size([64])\n",
      "layer1.2.bn1.running_mean \t torch.Size([64])\n",
      "layer1.2.bn1.running_var \t torch.Size([64])\n",
      "layer1.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.2.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.2.bn2.weight \t torch.Size([64])\n",
      "layer1.2.bn2.bias \t torch.Size([64])\n",
      "layer1.2.bn2.running_mean \t torch.Size([64])\n",
      "layer1.2.bn2.running_var \t torch.Size([64])\n",
      "layer1.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer1.2.conv3.weight \t torch.Size([256, 64, 1, 1])\n",
      "layer1.2.bn3.weight \t torch.Size([256])\n",
      "layer1.2.bn3.bias \t torch.Size([256])\n",
      "layer1.2.bn3.running_mean \t torch.Size([256])\n",
      "layer1.2.bn3.running_var \t torch.Size([256])\n",
      "layer1.2.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.conv1.weight \t torch.Size([128, 256, 1, 1])\n",
      "layer2.0.bn1.weight \t torch.Size([128])\n",
      "layer2.0.bn1.bias \t torch.Size([128])\n",
      "layer2.0.bn1.running_mean \t torch.Size([128])\n",
      "layer2.0.bn1.running_var \t torch.Size([128])\n",
      "layer2.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.0.bn2.weight \t torch.Size([128])\n",
      "layer2.0.bn2.bias \t torch.Size([128])\n",
      "layer2.0.bn2.running_mean \t torch.Size([128])\n",
      "layer2.0.bn2.running_var \t torch.Size([128])\n",
      "layer2.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.0.bn3.weight \t torch.Size([512])\n",
      "layer2.0.bn3.bias \t torch.Size([512])\n",
      "layer2.0.bn3.running_mean \t torch.Size([512])\n",
      "layer2.0.bn3.running_var \t torch.Size([512])\n",
      "layer2.0.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "layer2.0.downsample.1.weight \t torch.Size([512])\n",
      "layer2.0.downsample.1.bias \t torch.Size([512])\n",
      "layer2.0.downsample.1.running_mean \t torch.Size([512])\n",
      "layer2.0.downsample.1.running_var \t torch.Size([512])\n",
      "layer2.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer2.1.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.1.bn1.weight \t torch.Size([128])\n",
      "layer2.1.bn1.bias \t torch.Size([128])\n",
      "layer2.1.bn1.running_mean \t torch.Size([128])\n",
      "layer2.1.bn1.running_var \t torch.Size([128])\n",
      "layer2.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn2.weight \t torch.Size([128])\n",
      "layer2.1.bn2.bias \t torch.Size([128])\n",
      "layer2.1.bn2.running_mean \t torch.Size([128])\n",
      "layer2.1.bn2.running_var \t torch.Size([128])\n",
      "layer2.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.1.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.1.bn3.weight \t torch.Size([512])\n",
      "layer2.1.bn3.bias \t torch.Size([512])\n",
      "layer2.1.bn3.running_mean \t torch.Size([512])\n",
      "layer2.1.bn3.running_var \t torch.Size([512])\n",
      "layer2.1.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.2.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.2.bn1.weight \t torch.Size([128])\n",
      "layer2.2.bn1.bias \t torch.Size([128])\n",
      "layer2.2.bn1.running_mean \t torch.Size([128])\n",
      "layer2.2.bn1.running_var \t torch.Size([128])\n",
      "layer2.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.2.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.2.bn2.weight \t torch.Size([128])\n",
      "layer2.2.bn2.bias \t torch.Size([128])\n",
      "layer2.2.bn2.running_mean \t torch.Size([128])\n",
      "layer2.2.bn2.running_var \t torch.Size([128])\n",
      "layer2.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.2.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.2.bn3.weight \t torch.Size([512])\n",
      "layer2.2.bn3.bias \t torch.Size([512])\n",
      "layer2.2.bn3.running_mean \t torch.Size([512])\n",
      "layer2.2.bn3.running_var \t torch.Size([512])\n",
      "layer2.2.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.3.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.3.bn1.weight \t torch.Size([128])\n",
      "layer2.3.bn1.bias \t torch.Size([128])\n",
      "layer2.3.bn1.running_mean \t torch.Size([128])\n",
      "layer2.3.bn1.running_var \t torch.Size([128])\n",
      "layer2.3.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.3.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.3.bn2.weight \t torch.Size([128])\n",
      "layer2.3.bn2.bias \t torch.Size([128])\n",
      "layer2.3.bn2.running_mean \t torch.Size([128])\n",
      "layer2.3.bn2.running_var \t torch.Size([128])\n",
      "layer2.3.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.3.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.3.bn3.weight \t torch.Size([512])\n",
      "layer2.3.bn3.bias \t torch.Size([512])\n",
      "layer2.3.bn3.running_mean \t torch.Size([512])\n",
      "layer2.3.bn3.running_var \t torch.Size([512])\n",
      "layer2.3.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.conv1.weight \t torch.Size([256, 512, 1, 1])\n",
      "layer3.0.bn1.weight \t torch.Size([256])\n",
      "layer3.0.bn1.bias \t torch.Size([256])\n",
      "layer3.0.bn1.running_mean \t torch.Size([256])\n",
      "layer3.0.bn1.running_var \t torch.Size([256])\n",
      "layer3.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.0.bn2.weight \t torch.Size([256])\n",
      "layer3.0.bn2.bias \t torch.Size([256])\n",
      "layer3.0.bn2.running_mean \t torch.Size([256])\n",
      "layer3.0.bn2.running_var \t torch.Size([256])\n",
      "layer3.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.0.bn3.weight \t torch.Size([1024])\n",
      "layer3.0.bn3.bias \t torch.Size([1024])\n",
      "layer3.0.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.0.bn3.running_var \t torch.Size([1024])\n",
      "layer3.0.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.downsample.0.weight \t torch.Size([1024, 512, 1, 1])\n",
      "layer3.0.downsample.1.weight \t torch.Size([1024])\n",
      "layer3.0.downsample.1.bias \t torch.Size([1024])\n",
      "layer3.0.downsample.1.running_mean \t torch.Size([1024])\n",
      "layer3.0.downsample.1.running_var \t torch.Size([1024])\n",
      "layer3.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer3.1.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.1.bn1.weight \t torch.Size([256])\n",
      "layer3.1.bn1.bias \t torch.Size([256])\n",
      "layer3.1.bn1.running_mean \t torch.Size([256])\n",
      "layer3.1.bn1.running_var \t torch.Size([256])\n",
      "layer3.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn2.weight \t torch.Size([256])\n",
      "layer3.1.bn2.bias \t torch.Size([256])\n",
      "layer3.1.bn2.running_mean \t torch.Size([256])\n",
      "layer3.1.bn2.running_var \t torch.Size([256])\n",
      "layer3.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.1.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.1.bn3.weight \t torch.Size([1024])\n",
      "layer3.1.bn3.bias \t torch.Size([1024])\n",
      "layer3.1.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.1.bn3.running_var \t torch.Size([1024])\n",
      "layer3.1.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.2.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.2.bn1.weight \t torch.Size([256])\n",
      "layer3.2.bn1.bias \t torch.Size([256])\n",
      "layer3.2.bn1.running_mean \t torch.Size([256])\n",
      "layer3.2.bn1.running_var \t torch.Size([256])\n",
      "layer3.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.2.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.2.bn2.weight \t torch.Size([256])\n",
      "layer3.2.bn2.bias \t torch.Size([256])\n",
      "layer3.2.bn2.running_mean \t torch.Size([256])\n",
      "layer3.2.bn2.running_var \t torch.Size([256])\n",
      "layer3.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.2.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.2.bn3.weight \t torch.Size([1024])\n",
      "layer3.2.bn3.bias \t torch.Size([1024])\n",
      "layer3.2.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.2.bn3.running_var \t torch.Size([1024])\n",
      "layer3.2.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.3.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.3.bn1.weight \t torch.Size([256])\n",
      "layer3.3.bn1.bias \t torch.Size([256])\n",
      "layer3.3.bn1.running_mean \t torch.Size([256])\n",
      "layer3.3.bn1.running_var \t torch.Size([256])\n",
      "layer3.3.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.3.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.3.bn2.weight \t torch.Size([256])\n",
      "layer3.3.bn2.bias \t torch.Size([256])\n",
      "layer3.3.bn2.running_mean \t torch.Size([256])\n",
      "layer3.3.bn2.running_var \t torch.Size([256])\n",
      "layer3.3.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.3.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.3.bn3.weight \t torch.Size([1024])\n",
      "layer3.3.bn3.bias \t torch.Size([1024])\n",
      "layer3.3.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.3.bn3.running_var \t torch.Size([1024])\n",
      "layer3.3.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.4.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.4.bn1.weight \t torch.Size([256])\n",
      "layer3.4.bn1.bias \t torch.Size([256])\n",
      "layer3.4.bn1.running_mean \t torch.Size([256])\n",
      "layer3.4.bn1.running_var \t torch.Size([256])\n",
      "layer3.4.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.4.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.4.bn2.weight \t torch.Size([256])\n",
      "layer3.4.bn2.bias \t torch.Size([256])\n",
      "layer3.4.bn2.running_mean \t torch.Size([256])\n",
      "layer3.4.bn2.running_var \t torch.Size([256])\n",
      "layer3.4.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.4.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.4.bn3.weight \t torch.Size([1024])\n",
      "layer3.4.bn3.bias \t torch.Size([1024])\n",
      "layer3.4.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.4.bn3.running_var \t torch.Size([1024])\n",
      "layer3.4.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.5.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.5.bn1.weight \t torch.Size([256])\n",
      "layer3.5.bn1.bias \t torch.Size([256])\n",
      "layer3.5.bn1.running_mean \t torch.Size([256])\n",
      "layer3.5.bn1.running_var \t torch.Size([256])\n",
      "layer3.5.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.5.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.5.bn2.weight \t torch.Size([256])\n",
      "layer3.5.bn2.bias \t torch.Size([256])\n",
      "layer3.5.bn2.running_mean \t torch.Size([256])\n",
      "layer3.5.bn2.running_var \t torch.Size([256])\n",
      "layer3.5.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.5.conv3.weight \t torch.Size([1024, 256, 1, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer3.5.bn3.weight \t torch.Size([1024])\n",
      "layer3.5.bn3.bias \t torch.Size([1024])\n",
      "layer3.5.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.5.bn3.running_var \t torch.Size([1024])\n",
      "layer3.5.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.conv1.weight \t torch.Size([512, 1024, 1, 1])\n",
      "layer4.0.bn1.weight \t torch.Size([512])\n",
      "layer4.0.bn1.bias \t torch.Size([512])\n",
      "layer4.0.bn1.running_mean \t torch.Size([512])\n",
      "layer4.0.bn1.running_var \t torch.Size([512])\n",
      "layer4.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.0.bn2.weight \t torch.Size([512])\n",
      "layer4.0.bn2.bias \t torch.Size([512])\n",
      "layer4.0.bn2.running_mean \t torch.Size([512])\n",
      "layer4.0.bn2.running_var \t torch.Size([512])\n",
      "layer4.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.conv3.weight \t torch.Size([2048, 512, 1, 1])\n",
      "layer4.0.bn3.weight \t torch.Size([2048])\n",
      "layer4.0.bn3.bias \t torch.Size([2048])\n",
      "layer4.0.bn3.running_mean \t torch.Size([2048])\n",
      "layer4.0.bn3.running_var \t torch.Size([2048])\n",
      "layer4.0.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.downsample.0.weight \t torch.Size([2048, 1024, 1, 1])\n",
      "layer4.0.downsample.1.weight \t torch.Size([2048])\n",
      "layer4.0.downsample.1.bias \t torch.Size([2048])\n",
      "layer4.0.downsample.1.running_mean \t torch.Size([2048])\n",
      "layer4.0.downsample.1.running_var \t torch.Size([2048])\n",
      "layer4.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer4.1.conv1.weight \t torch.Size([512, 2048, 1, 1])\n",
      "layer4.1.bn1.weight \t torch.Size([512])\n",
      "layer4.1.bn1.bias \t torch.Size([512])\n",
      "layer4.1.bn1.running_mean \t torch.Size([512])\n",
      "layer4.1.bn1.running_var \t torch.Size([512])\n",
      "layer4.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer4.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn2.weight \t torch.Size([512])\n",
      "layer4.1.bn2.bias \t torch.Size([512])\n",
      "layer4.1.bn2.running_mean \t torch.Size([512])\n",
      "layer4.1.bn2.running_var \t torch.Size([512])\n",
      "layer4.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer4.1.conv3.weight \t torch.Size([2048, 512, 1, 1])\n",
      "layer4.1.bn3.weight \t torch.Size([2048])\n",
      "layer4.1.bn3.bias \t torch.Size([2048])\n",
      "layer4.1.bn3.running_mean \t torch.Size([2048])\n",
      "layer4.1.bn3.running_var \t torch.Size([2048])\n",
      "layer4.1.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer4.2.conv1.weight \t torch.Size([512, 2048, 1, 1])\n",
      "layer4.2.bn1.weight \t torch.Size([512])\n",
      "layer4.2.bn1.bias \t torch.Size([512])\n",
      "layer4.2.bn1.running_mean \t torch.Size([512])\n",
      "layer4.2.bn1.running_var \t torch.Size([512])\n",
      "layer4.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer4.2.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.2.bn2.weight \t torch.Size([512])\n",
      "layer4.2.bn2.bias \t torch.Size([512])\n",
      "layer4.2.bn2.running_mean \t torch.Size([512])\n",
      "layer4.2.bn2.running_var \t torch.Size([512])\n",
      "layer4.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer4.2.conv3.weight \t torch.Size([2048, 512, 1, 1])\n",
      "layer4.2.bn3.weight \t torch.Size([2048])\n",
      "layer4.2.bn3.bias \t torch.Size([2048])\n",
      "layer4.2.bn3.running_mean \t torch.Size([2048])\n",
      "layer4.2.bn3.running_var \t torch.Size([2048])\n",
      "layer4.2.bn3.num_batches_tracked \t torch.Size([])\n",
      "fc.weight \t torch.Size([120, 2048])\n",
      "fc.bias \t torch.Size([120])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [2552374102280, 2552374101240]}]\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validate Model\n",
    "\n",
    "def train_model(loader, model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    \n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    history  = {\n",
    "        'train_acc': [],\n",
    "        'train_los': [],\n",
    "        'valid_acc': [],\n",
    "        'valid_los': []\n",
    "    }\n",
    "    \n",
    "    dataset_sizes = {\n",
    "        'train': len(loader['train'].dataset),\n",
    "        'valid': len(loader['valid'].dataset)\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        for phase in ['train', 'valid']:\n",
    "            \n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            \n",
    "            for inputs, labels, iids in loader[phase]:\n",
    "                if use_gpu:\n",
    "                    inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs.data, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # statistic\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            \n",
    "            data_size = dataset_sizes[phase]\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_epoch_loss = running_loss / data_size\n",
    "                train_epoch_acc  = running_corrects / data_size\n",
    "            else:\n",
    "                valid_epoch_loss = running_loss / data_size\n",
    "                valid_epoch_acc  = running_corrects / data_size\n",
    "\n",
    "            if phase == 'valid' and valid_epoch_acc > best_acc:\n",
    "                best_acc = valid_epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        history['train_acc'].append(train_epoch_acc.item())\n",
    "        history['train_los'].append(train_epoch_loss)\n",
    "        history['valid_acc'].append(valid_epoch_acc.item())\n",
    "        history['valid_los'].append(valid_epoch_loss)\n",
    "        \n",
    "        fmt_str = (\n",
    "            'Epoch [{:3d}/{:3d}] train loss: {:.4f} acc: {:.4f}'\n",
    "            ', valid loss: {:.4f} acc: {:.4f}'\n",
    "        )\n",
    "        print(fmt_str.format(\n",
    "            epoch, num_epochs - 1, \n",
    "            train_epoch_loss, train_epoch_acc, valid_epoch_loss, valid_epoch_acc\n",
    "        ))\n",
    "\n",
    "    print('\\nBest val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_acc, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [  0/  2] train loss: 0.2223 acc: 0.3494, valid loss: 0.1329 acc: 0.6655\n",
      "Epoch [  1/  2] train loss: 0.1199 acc: 0.6863, valid loss: 0.0744 acc: 0.7971\n",
      "Epoch [  2/  2] train loss: 0.0834 acc: 0.7566, valid loss: 0.0566 acc: 0.8372\n",
      "\n",
      "Best val Acc: 0.837164\n",
      "Training time:   8.559859 minutes\n"
     ]
    }
   ],
   "source": [
    "NumEpochs = 3\n",
    "start_time = time.time()\n",
    "\n",
    "best_model, best_acc, history = train_model(\n",
    "    dataLoader, model, criterion, optimizer, exp_lr_scheduler, NumEpochs\n",
    ")\n",
    "    \n",
    "print('Training time: {:10f} minutes'.format((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'train_acc': [0.34939464926719666, 0.6863152384757996, 0.7566344738006592],\n",
      "  'train_los': [0.2222697974213531, 0.11994682410038289, 0.08337554707713184],\n",
      "  'valid_acc': [0.6655256748199463, 0.7970660328865051, 0.8371638059616089],\n",
      "  'valid_los': [0.1329476889304835, 0.07444079974461301, 0.05658155289139316]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(history, indent=2)\n",
    "\n",
    "train_acc = history['train_acc']\n",
    "train_los = history['train_los']\n",
    "valid_acc = history['valid_acc']\n",
    "valid_los = history['valid_los']\n",
    "\n",
    "# for i in range(len(history)):\n",
    "#     train_acc.append(history[i]['train_acc'])\n",
    "#     train_los.append(history[i]['train_loss'])\n",
    "#     valid_acc.append(history[i]['valid_acc'])\n",
    "#     valid_los.append(history[i]['valid_loss'])\n",
    "        \n",
    "def plot_lines(y1, y2, label=None, ax=None):\n",
    "    epochs = len(y1)\n",
    "    x = np.linspace(0, epochs, num=epochs)\n",
    "    ax.plot(x, y1, label=\"acc\")\n",
    "    ax.plot(x, y2, label=\"loss\")\n",
    "    # ax.set_title('training v.s. validate')\n",
    "    ax.set_title(label)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Percentage')\n",
    "    \n",
    "    \n",
    "# 1. Plot in same line, this would work\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "plot_lines(train_acc, train_los, 'training', ax1)\n",
    "plt.legend()\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "plot_lines(valid_acc, valid_los, 'validate', ax2)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "currDT = datetime.now()\n",
    "currStr = currDT.strftime(\"%Y%m%d-%H%M\")\n",
    "\n",
    "acc_str = int(best_acc * 100)\n",
    "fname_best_model = 'resnet50_{}_acc{}.pth'.format(currStr, acc_str)\n",
    "\n",
    "best_model_wts = best_model.state_dict()\n",
    "\n",
    "OutPath = Params['OutPath']\n",
    "best_model_out = join(OutPath, fname_best_model)\n",
    "\n",
    "torch.save(best_model_wts, best_model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"DataPath\": \"D:\\\\GitWork\\\\dog_breed\\\\data\",\n",
      "    \"OutPath\": \"D:\\\\GitWork\\\\dog_breed\\\\output\",\n",
      "    \"ProcPath\": \"D:\\\\GitWork\\\\dog_breed\\\\processed\",\n",
      "    \"PreTrainPath\": \"D:\\\\GitWork\\\\dog_breed\\\\pretrained\",\n",
      "    \"PreTrainFile\": \"\",\n",
      "    \"TestPath\": \"D:\\\\Dataset\\\\dog-breed-identification\\\\test\",\n",
      "    \"TrainPath\": \"D:\\\\Dataset\\\\dog-breed-identification\\\\train\",\n",
      "    \"CsvLabel\": \"labels.csv\",\n",
      "    \"BatchSize\": 16,\n",
      "    \"FracForTrain\": 0.8\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Save parameters to json file\n",
    "\n",
    "fname = 'Params_{}.json'.format(currStr)\n",
    "f_abspath = join(OutPath, fname)\n",
    "\n",
    "json_str = json.dumps(Params, indent=4)\n",
    "print(json_str)\n",
    "\n",
    "with open(f_abspath, 'w') as fout:\n",
    "    fout.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"scottish_deerhound\": 0,\n",
      "    \"maltese_dog\": 1,\n",
      "    \"afghan_hound\": 2,\n",
      "    \"entlebucher\": 3,\n",
      "    \"bernese_mountain_dog\": 4,\n",
      "    \"shih-tzu\": 5,\n",
      "    \"great_pyrenees\": 6,\n",
      "    \"pomeranian\": 7,\n",
      "    \"basenji\": 8,\n",
      "    \"samoyed\": 9,\n",
      "    \"airedale\": 10,\n",
      "    \"tibetan_terrier\": 11,\n",
      "    \"leonberg\": 12,\n",
      "    \"cairn\": 13,\n",
      "    \"beagle\": 14,\n",
      "    \"japanese_spaniel\": 15,\n",
      "    \"australian_terrier\": 16,\n",
      "    \"blenheim_spaniel\": 17,\n",
      "    \"miniature_pinscher\": 18,\n",
      "    \"irish_wolfhound\": 19,\n",
      "    \"lakeland_terrier\": 20,\n",
      "    \"saluki\": 21,\n",
      "    \"papillon\": 22,\n",
      "    \"whippet\": 23,\n",
      "    \"siberian_husky\": 24,\n",
      "    \"norwegian_elkhound\": 25,\n",
      "    \"pug\": 26,\n",
      "    \"chow\": 27,\n",
      "    \"italian_greyhound\": 28,\n",
      "    \"pembroke\": 29,\n",
      "    \"ibizan_hound\": 30,\n",
      "    \"border_terrier\": 31,\n",
      "    \"newfoundland\": 32,\n",
      "    \"lhasa\": 33,\n",
      "    \"silky_terrier\": 34,\n",
      "    \"bedlington_terrier\": 35,\n",
      "    \"dandie_dinmont\": 36,\n",
      "    \"irish_setter\": 37,\n",
      "    \"sealyham_terrier\": 38,\n",
      "    \"rhodesian_ridgeback\": 39,\n",
      "    \"old_english_sheepdog\": 40,\n",
      "    \"collie\": 41,\n",
      "    \"boston_bull\": 42,\n",
      "    \"english_foxhound\": 43,\n",
      "    \"bouvier_des_flandres\": 44,\n",
      "    \"african_hunting_dog\": 45,\n",
      "    \"schipperke\": 46,\n",
      "    \"kelpie\": 47,\n",
      "    \"weimaraner\": 48,\n",
      "    \"bloodhound\": 49,\n",
      "    \"bluetick\": 50,\n",
      "    \"saint_bernard\": 51,\n",
      "    \"labrador_retriever\": 52,\n",
      "    \"chesapeake_bay_retriever\": 53,\n",
      "    \"norfolk_terrier\": 54,\n",
      "    \"english_setter\": 55,\n",
      "    \"wire-haired_fox_terrier\": 56,\n",
      "    \"kerry_blue_terrier\": 57,\n",
      "    \"scotch_terrier\": 58,\n",
      "    \"yorkshire_terrier\": 59,\n",
      "    \"groenendael\": 60,\n",
      "    \"greater_swiss_mountain_dog\": 61,\n",
      "    \"irish_terrier\": 62,\n",
      "    \"basset\": 63,\n",
      "    \"keeshond\": 64,\n",
      "    \"west_highland_white_terrier\": 65,\n",
      "    \"gordon_setter\": 66,\n",
      "    \"malamute\": 67,\n",
      "    \"affenpinscher\": 68,\n",
      "    \"toy_poodle\": 69,\n",
      "    \"clumber\": 70,\n",
      "    \"mexican_hairless\": 71,\n",
      "    \"dingo\": 72,\n",
      "    \"standard_poodle\": 73,\n",
      "    \"miniature_poodle\": 74,\n",
      "    \"staffordshire_bullterrier\": 75,\n",
      "    \"welsh_springer_spaniel\": 76,\n",
      "    \"toy_terrier\": 77,\n",
      "    \"sussex_spaniel\": 78,\n",
      "    \"norwich_terrier\": 79,\n",
      "    \"appenzeller\": 80,\n",
      "    \"irish_water_spaniel\": 81,\n",
      "    \"miniature_schnauzer\": 82,\n",
      "    \"black-and-tan_coonhound\": 83,\n",
      "    \"cardigan\": 84,\n",
      "    \"dhole\": 85,\n",
      "    \"shetland_sheepdog\": 86,\n",
      "    \"rottweiler\": 87,\n",
      "    \"english_springer\": 88,\n",
      "    \"great_dane\": 89,\n",
      "    \"german_short-haired_pointer\": 90,\n",
      "    \"boxer\": 91,\n",
      "    \"bull_mastiff\": 92,\n",
      "    \"borzoi\": 93,\n",
      "    \"pekinese\": 94,\n",
      "    \"cocker_spaniel\": 95,\n",
      "    \"american_staffordshire_terrier\": 96,\n",
      "    \"doberman\": 97,\n",
      "    \"brittany_spaniel\": 98,\n",
      "    \"malinois\": 99,\n",
      "    \"standard_schnauzer\": 100,\n",
      "    \"flat-coated_retriever\": 101,\n",
      "    \"redbone\": 102,\n",
      "    \"border_collie\": 103,\n",
      "    \"curly-coated_retriever\": 104,\n",
      "    \"kuvasz\": 105,\n",
      "    \"chihuahua\": 106,\n",
      "    \"soft-coated_wheaten_terrier\": 107,\n",
      "    \"french_bulldog\": 108,\n",
      "    \"vizsla\": 109,\n",
      "    \"tibetan_mastiff\": 110,\n",
      "    \"german_shepherd\": 111,\n",
      "    \"giant_schnauzer\": 112,\n",
      "    \"walker_hound\": 113,\n",
      "    \"otterhound\": 114,\n",
      "    \"golden_retriever\": 115,\n",
      "    \"brabancon_griffon\": 116,\n",
      "    \"komondor\": 117,\n",
      "    \"briard\": 118,\n",
      "    \"eskimo_dog\": 119\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Save breed dict to json file\n",
    "fname = 'BreedDict_{}.json'.format(currStr)\n",
    "f_abspath = join(OutPath, fname)\n",
    "\n",
    "json_str = json.dumps(dict_bid_fw, indent=4)\n",
    "print(json_str)\n",
    "\n",
    "with open(f_abspath, 'w') as fout:\n",
    "    fout.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
