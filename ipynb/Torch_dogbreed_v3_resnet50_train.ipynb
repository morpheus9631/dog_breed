{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.5.1\n",
      "Torchvision Version:  0.6.1\n"
     ]
    }
   ],
   "source": [
    "# From: https://www.kaggle.com/c/dog-breed-identification/data\n",
    "# Author: Morpheus Hsieh (morpheus.hsieh@gmail.com)\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import os, sys\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from os import listdir\n",
    "from os.path import join, exists\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "{\n",
      "  'DataPath'    : 'D:\\GitWork\\dog_breed\\data',\n",
      "  'OutPath'     : 'D:\\GitWork\\dog_breed\\output',\n",
      "  'ProcPath'    : 'D:\\GitWork\\dog_breed\\processed',\n",
      "  'PreTrainPath': 'D:\\GitWork\\dog_breed\\pretrained',\n",
      "  'PreTrainFile': 'resnet50_20200923-1604_acc79.pth',\n",
      "  'LoadPreModel': False,\n",
      "  'TestPath'    : 'D:\\GitWork\\dog_breed\\data\\test',\n",
      "  'TrainPath'   : 'D:\\GitWork\\dog_breed\\data\\train',\n",
      "  'CsvLabel'    : 'labels.csv',\n",
      "  'BatchSize'   : 16,\n",
      "  'FracForTrain': 0.8\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "Params = {\n",
    "    'DataPath'    : r'D:\\GitWork\\dog_breed\\data',\n",
    "    'OutPath'     : r'D:\\GitWork\\dog_breed\\output',\n",
    "    'ProcPath'    : r'D:\\GitWork\\dog_breed\\processed',\n",
    "    'PreTrainPath': r'D:\\GitWork\\dog_breed\\pretrained',\n",
    "    'PreTrainFile': 'resnet50_20200923-1604_acc79.pth',\n",
    "    'LoadPreModel': False,\n",
    "    'TestPath'    : r'D:\\GitWork\\dog_breed\\data\\test',\n",
    "    'TrainPath'   : r'D:\\GitWork\\dog_breed\\data\\train',\n",
    "    'CsvLabel'    : 'labels.csv',\n",
    "    'BatchSize'   : 16,\n",
    "    'FracForTrain': 0.8\n",
    "}\n",
    "\n",
    "\n",
    "def prettyDict(dic, indent=2):\n",
    "    array = []\n",
    "    key_maxlen = 0\n",
    "    item_cnt = 0\n",
    "    item_size = len(dic)\n",
    "    split_str = ': '\n",
    "    \n",
    "    for key, val in dic.items():\n",
    "        if key_maxlen < len(str(key)): \n",
    "            key_maxlen = len(str(key))\n",
    "        \n",
    "        tmpstr = ''\n",
    "        tmpstr += f\"'{key}'\" if isinstance(key, str) else f\"{key}\"\n",
    "        tmpstr += split_str\n",
    "        tmpstr += f\"'{val}'\" if isinstance(val, str) else f\"{val}\"\n",
    "\n",
    "        item_cnt += 1\n",
    "        if item_cnt < item_size: tmpstr += ','\n",
    "        array.append(tmpstr)\n",
    "    \n",
    "    for i in range(len(array)):\n",
    "        inStr = array[i]\n",
    "        ary = inStr.split(split_str)\n",
    "        key = ary[0].ljust(key_maxlen+2)\n",
    "        val = ary[1]\n",
    "        array[i] = (' '*indent) + key + ': ' + val\n",
    "        \n",
    "    outstr = '{\\n' + '\\n'.join(array) + '\\n}'\n",
    "    return outstr\n",
    "\n",
    "outstr = prettyDict(Params)\n",
    "print('Parameters:')\n",
    "print(outstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10222 entries, 0 to 10221\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      10222 non-null  object\n",
      " 1   breed   10222 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 159.8+ KB\n",
      "None\n",
      "\n",
      "                                 id             breed\n",
      "0  000bec180eb18c7604dcecc8fe0dba07       boston_bull\n",
      "1  001513dfcb2ffafc82cccf4d8bbaba97             dingo\n",
      "2  001cdf01b096e06d78e9e5112d419397          pekinese\n",
      "3  00214f311d5d2247d5dfe4fe24b2303d          bluetick\n",
      "4  0021f9ceb3235effd7fcde7f7538ed62  golden_retriever\n"
     ]
    }
   ],
   "source": [
    "# Read breed information from csv\n",
    "DataPath = Params.get('DataPath')\n",
    "csv_labels = Params.get('CsvLabel')\n",
    "f_abspath = join(DataPath, csv_labels)\n",
    "\n",
    "df_labels = pd.read_csv(f_abspath)\n",
    "\n",
    "print(df_labels.info())\n",
    "print()\n",
    "print(df_labels.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120 entries, 0 to 119\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   breed_id  120 non-null    int64 \n",
      " 1   breed     120 non-null    object\n",
      " 2   count     120 non-null    int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 2.9+ KB\n",
      "None\n",
      "\n",
      "   breed_id                 breed  count\n",
      "0         0    scottish_deerhound    126\n",
      "1         1           maltese_dog    117\n",
      "2         2          afghan_hound    116\n",
      "3         3           entlebucher    115\n",
      "4         4  bernese_mountain_dog    114\n",
      "\n",
      "Num classes: 120\n"
     ]
    }
   ],
   "source": [
    "# Count all breeds\n",
    "def countBreeds(df):\n",
    "    df1 = df_labels.groupby(\"breed\")[\"id\"].count().reset_index(name=\"count\")\n",
    "    df1 = df1.sort_values(by='count', ascending=False).reset_index(drop=True)\n",
    "    df1.insert(0, 'breed_id', df1.index)\n",
    "    return df1\n",
    "\n",
    "df_breeds = countBreeds(df_labels)\n",
    "print(df_breeds.info())\n",
    "print()\n",
    "print(df_breeds.head())\n",
    "\n",
    "NumClasses = int(df_breeds.shape[0])\n",
    "print('\\nNum classes:', NumClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10222 entries, 0 to 10221\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   image     10222 non-null  object\n",
      " 1   breed_id  10222 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 159.8+ KB\n",
      "None\n",
      "                                               image  breed_id\n",
      "0  D:\\GitWork\\dog_breed\\data\\train\\000bec180eb18c...        42\n",
      "1  D:\\GitWork\\dog_breed\\data\\train\\001513dfcb2ffa...        72\n",
      "2  D:\\GitWork\\dog_breed\\data\\train\\001cdf01b096e0...        94\n",
      "3  D:\\GitWork\\dog_breed\\data\\train\\00214f311d5d22...        50\n",
      "4  D:\\GitWork\\dog_breed\\data\\train\\0021f9ceb3235e...       115\n"
     ]
    }
   ],
   "source": [
    "# Process labels\n",
    "\n",
    "dict_bid_fw = dict(df_breeds[['breed', 'breed_id']].values)\n",
    "# print(dict_bid_fw)\n",
    "\n",
    "dict_bid_bw = dict(df_breeds[['breed_id', 'breed']].values)\n",
    "# print(dict_bid_bw)\n",
    "\n",
    "# Build processed labels file\n",
    "df_data = pd.DataFrame(columns=['image', 'breed_id'])\n",
    "df_data['breed_id'] = df_labels.breed.map(dict_bid_fw)\n",
    "\n",
    "TranPath = Params['TrainPath']\n",
    "\n",
    "df_data['image'] = df_labels.apply (\n",
    "    lambda row: join(TranPath, row['id']+'.jpg') \\\n",
    "    if exists(join(TranPath, row['id']+'.jpg')) else None, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(df_data.info())\n",
    "print(df_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataSet size: {'train': 8177, 'valid': 2045}\n",
      "\n",
      "Image shape: torch.Size([16, 3, 224, 224])\n",
      "Label shape: torch.Size([16])\n",
      "\n",
      "Image iid:\n",
      "  4d07baea3d7e78ff59ba05dda3f3932a\n",
      "  b520efcc428dc8666a138d1f6eb65853\n",
      "  5aef2850c1ad620dafdeb39745316d81\n",
      "  8c1304c22e356290cae941c3b29cddff\n",
      "  3f4677ac28ed7d52a781dc9a3c378eaa\n",
      "  5eaaffc76b998febfb5e6800de615b44\n",
      "  3218df2f90a507530be3f1c2a5b5d899\n",
      "  a42494c29d0cd8099e0da27de9cc9373\n",
      "  1ebf07f4fc0f44b6befec9aa3cd8adbb\n",
      "  57c957cab29a52ac2801d23027fbf414\n",
      "  52c9ab0f6d9f27daaf3641f86c019cae\n",
      "  56ddc4e4e63361693d88bdcb0a5cc3ae\n",
      "  1d114c4409c9cba464f762b11ce47d57\n",
      "  c8edd4f20e6fee048cd1a61067a1a4a3\n",
      "  0daad4c8568dd89f295f6c3837b772df\n",
      "  04e3bb77c15c18fc1ac101d86d612f46\n",
      "\n",
      "Image shape: torch.Size([3, 224, 224])\n",
      "\n",
      "tensor([[[ 0.2796, -0.1143, -1.0048,  ...,  1.3413,  1.5297,  1.7523],\n",
      "         [ 0.3994, -0.0629, -0.5596,  ...,  1.4954,  1.6838,  1.7523],\n",
      "         [ 0.5878,  0.1426, -0.0287,  ...,  1.3242,  1.7009,  1.8379],\n",
      "         ...,\n",
      "         [ 0.0912, -0.0801, -0.3198,  ...,  0.3652,  0.3652,  0.4508],\n",
      "         [ 0.1597,  0.0569,  0.1597,  ...,  0.5707,  0.2796,  0.2967],\n",
      "         [ 0.3994,  0.2624,  0.5193,  ...,  0.8104,  0.3994,  0.3652]],\n",
      "\n",
      "        [[ 0.4503,  0.0476, -0.8627,  ...,  1.5007,  1.6933,  1.9209],\n",
      "         [ 0.5728,  0.1001, -0.4076,  ...,  1.6583,  1.8508,  1.9209],\n",
      "         [ 0.7479,  0.3102,  0.1176,  ...,  1.4832,  1.8683,  2.0084],\n",
      "         ...,\n",
      "         [ 0.1352, -0.0399, -0.2850,  ...,  0.3277,  0.3277,  0.4153],\n",
      "         [ 0.2052,  0.1001,  0.2052,  ...,  0.5378,  0.2402,  0.2577],\n",
      "         [ 0.4503,  0.3102,  0.5728,  ...,  0.7829,  0.3627,  0.3277]],\n",
      "\n",
      "        [[ 0.4439,  0.0431, -0.8633,  ...,  1.7163,  1.9080,  2.1346],\n",
      "         [ 0.5834,  0.1128, -0.3927,  ...,  1.8731,  2.0648,  2.1346],\n",
      "         [ 0.7925,  0.3568,  0.1825,  ...,  1.6988,  2.0823,  2.2217],\n",
      "         ...,\n",
      "         [ 0.3045,  0.1302, -0.1138,  ...,  0.5311,  0.5311,  0.6182],\n",
      "         [ 0.3742,  0.2696,  0.3742,  ...,  0.7402,  0.4439,  0.4614],\n",
      "         [ 0.6182,  0.4788,  0.7402,  ...,  0.9842,  0.5659,  0.5311]]])\n",
      "\n",
      "Labels: tensor(15)\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean = [0.485, 0.456, 0.406],\n",
    "        std  = [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "class myDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, phase='train', frac=0.8, transform=None):\n",
    "        \n",
    "        num_rows = df.shape[0]\n",
    "        train_len = int(float(frac) * float(num_rows))\n",
    "        valid_len = num_rows - train_len\n",
    "        \n",
    "        data = df.head(train_len) if phase=='train' else df.tail(valid_len)\n",
    "        self.images = data['image'].tolist()\n",
    "        self.labels = data['breed_id'].tolist()\n",
    "\n",
    "        self.transform = transform\n",
    "        self.len = len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        f_abspath = self.images[index]\n",
    "        img_pil = Image.open(f_abspath)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img_pil)\n",
    "\n",
    "        lbl = int(self.labels[index])\n",
    "        iid = os.path.split(f_abspath)[1].replace('.jpg', '')\n",
    "        \n",
    "        return [img, lbl, iid]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "\n",
    "frac = Params['FracForTrain']\n",
    "\n",
    "phases = ['train', 'valid']\n",
    "dataSet = { \n",
    "    x: myDataset(df_data, phase=x, frac=frac, transform=transform) for x in phases \n",
    "}\n",
    "\n",
    "BatchSize = Params['BatchSize']\n",
    "dataLoader = {\n",
    "    x: DataLoader(dataSet[x], batch_size=BatchSize, shuffle=True) for x in phases\n",
    "}\n",
    "\n",
    "dataSizes = { x: len(dataSet[x]) for x in phases }\n",
    "print('DataSet size:', dataSizes)\n",
    "\n",
    "trainLoader = dataLoader['train']\n",
    "imgs, lbls, iids = next(iter(trainLoader))\n",
    "print('\\nImage shape:', imgs.size())\n",
    "print('Label shape:', lbls.size())\n",
    "\n",
    "print('\\nImage iid:')\n",
    "id_list = [''.join(iid) for iid in iids]\n",
    "print('  '+'\\n  '.join(id_list))\n",
    "\n",
    "img = imgs[0]\n",
    "print('\\nImage shape:', img.shape)\n",
    "print(); print(img)\n",
    "\n",
    "print('\\nLabels:', lbls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Use GPU for train\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=120, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Build Model\n",
    "def buildModel(use_gpu, numClasses, preTrainModel=None):\n",
    "    model = models.resnet50(pretrained=True)\n",
    "\n",
    "    # freeze all model parameters\n",
    "    for param in model.parameters():\n",
    "        model.requires_grad = False\n",
    "\n",
    "    # new final layer with 16 classes\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, numClasses)\n",
    "    \n",
    "    if preTrainModel is not None:\n",
    "        model.load_state_dict(preTrainModel)\n",
    "        \n",
    "    if use_gpu: \n",
    "        model = model.cuda()\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "pretrain_model = None\n",
    "\n",
    "load_premodel = Params['LoadPreModel']\n",
    "if load_premodel > 0:\n",
    "    path  = Params['PreTrainPath']\n",
    "    fname = Params['PreTrainFile']\n",
    "    pretrain_model = torch.load(join(path, fname))\n",
    "\n",
    "model = buildModel(use_gpu, NumClasses, preTrainModel=pretrain_model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([64, 3, 7, 7])\n",
      "bn1.weight \t torch.Size([64])\n",
      "bn1.bias \t torch.Size([64])\n",
      "bn1.running_mean \t torch.Size([64])\n",
      "bn1.running_var \t torch.Size([64])\n",
      "bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.conv1.weight \t torch.Size([64, 64, 1, 1])\n",
      "layer1.0.bn1.weight \t torch.Size([64])\n",
      "layer1.0.bn1.bias \t torch.Size([64])\n",
      "layer1.0.bn1.running_mean \t torch.Size([64])\n",
      "layer1.0.bn1.running_var \t torch.Size([64])\n",
      "layer1.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn2.weight \t torch.Size([64])\n",
      "layer1.0.bn2.bias \t torch.Size([64])\n",
      "layer1.0.bn2.running_mean \t torch.Size([64])\n",
      "layer1.0.bn2.running_var \t torch.Size([64])\n",
      "layer1.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.conv3.weight \t torch.Size([256, 64, 1, 1])\n",
      "layer1.0.bn3.weight \t torch.Size([256])\n",
      "layer1.0.bn3.bias \t torch.Size([256])\n",
      "layer1.0.bn3.running_mean \t torch.Size([256])\n",
      "layer1.0.bn3.running_var \t torch.Size([256])\n",
      "layer1.0.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.downsample.0.weight \t torch.Size([256, 64, 1, 1])\n",
      "layer1.0.downsample.1.weight \t torch.Size([256])\n",
      "layer1.0.downsample.1.bias \t torch.Size([256])\n",
      "layer1.0.downsample.1.running_mean \t torch.Size([256])\n",
      "layer1.0.downsample.1.running_var \t torch.Size([256])\n",
      "layer1.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer1.1.conv1.weight \t torch.Size([64, 256, 1, 1])\n",
      "layer1.1.bn1.weight \t torch.Size([64])\n",
      "layer1.1.bn1.bias \t torch.Size([64])\n",
      "layer1.1.bn1.running_mean \t torch.Size([64])\n",
      "layer1.1.bn1.running_var \t torch.Size([64])\n",
      "layer1.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn2.weight \t torch.Size([64])\n",
      "layer1.1.bn2.bias \t torch.Size([64])\n",
      "layer1.1.bn2.running_mean \t torch.Size([64])\n",
      "layer1.1.bn2.running_var \t torch.Size([64])\n",
      "layer1.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer1.1.conv3.weight \t torch.Size([256, 64, 1, 1])\n",
      "layer1.1.bn3.weight \t torch.Size([256])\n",
      "layer1.1.bn3.bias \t torch.Size([256])\n",
      "layer1.1.bn3.running_mean \t torch.Size([256])\n",
      "layer1.1.bn3.running_var \t torch.Size([256])\n",
      "layer1.1.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer1.2.conv1.weight \t torch.Size([64, 256, 1, 1])\n",
      "layer1.2.bn1.weight \t torch.Size([64])\n",
      "layer1.2.bn1.bias \t torch.Size([64])\n",
      "layer1.2.bn1.running_mean \t torch.Size([64])\n",
      "layer1.2.bn1.running_var \t torch.Size([64])\n",
      "layer1.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.2.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.2.bn2.weight \t torch.Size([64])\n",
      "layer1.2.bn2.bias \t torch.Size([64])\n",
      "layer1.2.bn2.running_mean \t torch.Size([64])\n",
      "layer1.2.bn2.running_var \t torch.Size([64])\n",
      "layer1.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer1.2.conv3.weight \t torch.Size([256, 64, 1, 1])\n",
      "layer1.2.bn3.weight \t torch.Size([256])\n",
      "layer1.2.bn3.bias \t torch.Size([256])\n",
      "layer1.2.bn3.running_mean \t torch.Size([256])\n",
      "layer1.2.bn3.running_var \t torch.Size([256])\n",
      "layer1.2.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.conv1.weight \t torch.Size([128, 256, 1, 1])\n",
      "layer2.0.bn1.weight \t torch.Size([128])\n",
      "layer2.0.bn1.bias \t torch.Size([128])\n",
      "layer2.0.bn1.running_mean \t torch.Size([128])\n",
      "layer2.0.bn1.running_var \t torch.Size([128])\n",
      "layer2.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.0.bn2.weight \t torch.Size([128])\n",
      "layer2.0.bn2.bias \t torch.Size([128])\n",
      "layer2.0.bn2.running_mean \t torch.Size([128])\n",
      "layer2.0.bn2.running_var \t torch.Size([128])\n",
      "layer2.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.0.bn3.weight \t torch.Size([512])\n",
      "layer2.0.bn3.bias \t torch.Size([512])\n",
      "layer2.0.bn3.running_mean \t torch.Size([512])\n",
      "layer2.0.bn3.running_var \t torch.Size([512])\n",
      "layer2.0.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "layer2.0.downsample.1.weight \t torch.Size([512])\n",
      "layer2.0.downsample.1.bias \t torch.Size([512])\n",
      "layer2.0.downsample.1.running_mean \t torch.Size([512])\n",
      "layer2.0.downsample.1.running_var \t torch.Size([512])\n",
      "layer2.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer2.1.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.1.bn1.weight \t torch.Size([128])\n",
      "layer2.1.bn1.bias \t torch.Size([128])\n",
      "layer2.1.bn1.running_mean \t torch.Size([128])\n",
      "layer2.1.bn1.running_var \t torch.Size([128])\n",
      "layer2.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn2.weight \t torch.Size([128])\n",
      "layer2.1.bn2.bias \t torch.Size([128])\n",
      "layer2.1.bn2.running_mean \t torch.Size([128])\n",
      "layer2.1.bn2.running_var \t torch.Size([128])\n",
      "layer2.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.1.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.1.bn3.weight \t torch.Size([512])\n",
      "layer2.1.bn3.bias \t torch.Size([512])\n",
      "layer2.1.bn3.running_mean \t torch.Size([512])\n",
      "layer2.1.bn3.running_var \t torch.Size([512])\n",
      "layer2.1.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.2.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.2.bn1.weight \t torch.Size([128])\n",
      "layer2.2.bn1.bias \t torch.Size([128])\n",
      "layer2.2.bn1.running_mean \t torch.Size([128])\n",
      "layer2.2.bn1.running_var \t torch.Size([128])\n",
      "layer2.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.2.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.2.bn2.weight \t torch.Size([128])\n",
      "layer2.2.bn2.bias \t torch.Size([128])\n",
      "layer2.2.bn2.running_mean \t torch.Size([128])\n",
      "layer2.2.bn2.running_var \t torch.Size([128])\n",
      "layer2.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.2.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.2.bn3.weight \t torch.Size([512])\n",
      "layer2.2.bn3.bias \t torch.Size([512])\n",
      "layer2.2.bn3.running_mean \t torch.Size([512])\n",
      "layer2.2.bn3.running_var \t torch.Size([512])\n",
      "layer2.2.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.3.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.3.bn1.weight \t torch.Size([128])\n",
      "layer2.3.bn1.bias \t torch.Size([128])\n",
      "layer2.3.bn1.running_mean \t torch.Size([128])\n",
      "layer2.3.bn1.running_var \t torch.Size([128])\n",
      "layer2.3.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.3.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.3.bn2.weight \t torch.Size([128])\n",
      "layer2.3.bn2.bias \t torch.Size([128])\n",
      "layer2.3.bn2.running_mean \t torch.Size([128])\n",
      "layer2.3.bn2.running_var \t torch.Size([128])\n",
      "layer2.3.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.3.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.3.bn3.weight \t torch.Size([512])\n",
      "layer2.3.bn3.bias \t torch.Size([512])\n",
      "layer2.3.bn3.running_mean \t torch.Size([512])\n",
      "layer2.3.bn3.running_var \t torch.Size([512])\n",
      "layer2.3.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.conv1.weight \t torch.Size([256, 512, 1, 1])\n",
      "layer3.0.bn1.weight \t torch.Size([256])\n",
      "layer3.0.bn1.bias \t torch.Size([256])\n",
      "layer3.0.bn1.running_mean \t torch.Size([256])\n",
      "layer3.0.bn1.running_var \t torch.Size([256])\n",
      "layer3.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.0.bn2.weight \t torch.Size([256])\n",
      "layer3.0.bn2.bias \t torch.Size([256])\n",
      "layer3.0.bn2.running_mean \t torch.Size([256])\n",
      "layer3.0.bn2.running_var \t torch.Size([256])\n",
      "layer3.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.0.bn3.weight \t torch.Size([1024])\n",
      "layer3.0.bn3.bias \t torch.Size([1024])\n",
      "layer3.0.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.0.bn3.running_var \t torch.Size([1024])\n",
      "layer3.0.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.downsample.0.weight \t torch.Size([1024, 512, 1, 1])\n",
      "layer3.0.downsample.1.weight \t torch.Size([1024])\n",
      "layer3.0.downsample.1.bias \t torch.Size([1024])\n",
      "layer3.0.downsample.1.running_mean \t torch.Size([1024])\n",
      "layer3.0.downsample.1.running_var \t torch.Size([1024])\n",
      "layer3.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer3.1.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.1.bn1.weight \t torch.Size([256])\n",
      "layer3.1.bn1.bias \t torch.Size([256])\n",
      "layer3.1.bn1.running_mean \t torch.Size([256])\n",
      "layer3.1.bn1.running_var \t torch.Size([256])\n",
      "layer3.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn2.weight \t torch.Size([256])\n",
      "layer3.1.bn2.bias \t torch.Size([256])\n",
      "layer3.1.bn2.running_mean \t torch.Size([256])\n",
      "layer3.1.bn2.running_var \t torch.Size([256])\n",
      "layer3.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.1.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.1.bn3.weight \t torch.Size([1024])\n",
      "layer3.1.bn3.bias \t torch.Size([1024])\n",
      "layer3.1.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.1.bn3.running_var \t torch.Size([1024])\n",
      "layer3.1.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.2.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.2.bn1.weight \t torch.Size([256])\n",
      "layer3.2.bn1.bias \t torch.Size([256])\n",
      "layer3.2.bn1.running_mean \t torch.Size([256])\n",
      "layer3.2.bn1.running_var \t torch.Size([256])\n",
      "layer3.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.2.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.2.bn2.weight \t torch.Size([256])\n",
      "layer3.2.bn2.bias \t torch.Size([256])\n",
      "layer3.2.bn2.running_mean \t torch.Size([256])\n",
      "layer3.2.bn2.running_var \t torch.Size([256])\n",
      "layer3.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.2.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.2.bn3.weight \t torch.Size([1024])\n",
      "layer3.2.bn3.bias \t torch.Size([1024])\n",
      "layer3.2.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.2.bn3.running_var \t torch.Size([1024])\n",
      "layer3.2.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.3.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.3.bn1.weight \t torch.Size([256])\n",
      "layer3.3.bn1.bias \t torch.Size([256])\n",
      "layer3.3.bn1.running_mean \t torch.Size([256])\n",
      "layer3.3.bn1.running_var \t torch.Size([256])\n",
      "layer3.3.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.3.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.3.bn2.weight \t torch.Size([256])\n",
      "layer3.3.bn2.bias \t torch.Size([256])\n",
      "layer3.3.bn2.running_mean \t torch.Size([256])\n",
      "layer3.3.bn2.running_var \t torch.Size([256])\n",
      "layer3.3.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.3.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.3.bn3.weight \t torch.Size([1024])\n",
      "layer3.3.bn3.bias \t torch.Size([1024])\n",
      "layer3.3.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.3.bn3.running_var \t torch.Size([1024])\n",
      "layer3.3.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.4.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.4.bn1.weight \t torch.Size([256])\n",
      "layer3.4.bn1.bias \t torch.Size([256])\n",
      "layer3.4.bn1.running_mean \t torch.Size([256])\n",
      "layer3.4.bn1.running_var \t torch.Size([256])\n",
      "layer3.4.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.4.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.4.bn2.weight \t torch.Size([256])\n",
      "layer3.4.bn2.bias \t torch.Size([256])\n",
      "layer3.4.bn2.running_mean \t torch.Size([256])\n",
      "layer3.4.bn2.running_var \t torch.Size([256])\n",
      "layer3.4.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.4.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.4.bn3.weight \t torch.Size([1024])\n",
      "layer3.4.bn3.bias \t torch.Size([1024])\n",
      "layer3.4.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.4.bn3.running_var \t torch.Size([1024])\n",
      "layer3.4.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.5.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.5.bn1.weight \t torch.Size([256])\n",
      "layer3.5.bn1.bias \t torch.Size([256])\n",
      "layer3.5.bn1.running_mean \t torch.Size([256])\n",
      "layer3.5.bn1.running_var \t torch.Size([256])\n",
      "layer3.5.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.5.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.5.bn2.weight \t torch.Size([256])\n",
      "layer3.5.bn2.bias \t torch.Size([256])\n",
      "layer3.5.bn2.running_mean \t torch.Size([256])\n",
      "layer3.5.bn2.running_var \t torch.Size([256])\n",
      "layer3.5.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.5.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.5.bn3.weight \t torch.Size([1024])\n",
      "layer3.5.bn3.bias \t torch.Size([1024])\n",
      "layer3.5.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.5.bn3.running_var \t torch.Size([1024])\n",
      "layer3.5.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.conv1.weight \t torch.Size([512, 1024, 1, 1])\n",
      "layer4.0.bn1.weight \t torch.Size([512])\n",
      "layer4.0.bn1.bias \t torch.Size([512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer4.0.bn1.running_mean \t torch.Size([512])\n",
      "layer4.0.bn1.running_var \t torch.Size([512])\n",
      "layer4.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.0.bn2.weight \t torch.Size([512])\n",
      "layer4.0.bn2.bias \t torch.Size([512])\n",
      "layer4.0.bn2.running_mean \t torch.Size([512])\n",
      "layer4.0.bn2.running_var \t torch.Size([512])\n",
      "layer4.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.conv3.weight \t torch.Size([2048, 512, 1, 1])\n",
      "layer4.0.bn3.weight \t torch.Size([2048])\n",
      "layer4.0.bn3.bias \t torch.Size([2048])\n",
      "layer4.0.bn3.running_mean \t torch.Size([2048])\n",
      "layer4.0.bn3.running_var \t torch.Size([2048])\n",
      "layer4.0.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.downsample.0.weight \t torch.Size([2048, 1024, 1, 1])\n",
      "layer4.0.downsample.1.weight \t torch.Size([2048])\n",
      "layer4.0.downsample.1.bias \t torch.Size([2048])\n",
      "layer4.0.downsample.1.running_mean \t torch.Size([2048])\n",
      "layer4.0.downsample.1.running_var \t torch.Size([2048])\n",
      "layer4.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer4.1.conv1.weight \t torch.Size([512, 2048, 1, 1])\n",
      "layer4.1.bn1.weight \t torch.Size([512])\n",
      "layer4.1.bn1.bias \t torch.Size([512])\n",
      "layer4.1.bn1.running_mean \t torch.Size([512])\n",
      "layer4.1.bn1.running_var \t torch.Size([512])\n",
      "layer4.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer4.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn2.weight \t torch.Size([512])\n",
      "layer4.1.bn2.bias \t torch.Size([512])\n",
      "layer4.1.bn2.running_mean \t torch.Size([512])\n",
      "layer4.1.bn2.running_var \t torch.Size([512])\n",
      "layer4.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer4.1.conv3.weight \t torch.Size([2048, 512, 1, 1])\n",
      "layer4.1.bn3.weight \t torch.Size([2048])\n",
      "layer4.1.bn3.bias \t torch.Size([2048])\n",
      "layer4.1.bn3.running_mean \t torch.Size([2048])\n",
      "layer4.1.bn3.running_var \t torch.Size([2048])\n",
      "layer4.1.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer4.2.conv1.weight \t torch.Size([512, 2048, 1, 1])\n",
      "layer4.2.bn1.weight \t torch.Size([512])\n",
      "layer4.2.bn1.bias \t torch.Size([512])\n",
      "layer4.2.bn1.running_mean \t torch.Size([512])\n",
      "layer4.2.bn1.running_var \t torch.Size([512])\n",
      "layer4.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer4.2.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.2.bn2.weight \t torch.Size([512])\n",
      "layer4.2.bn2.bias \t torch.Size([512])\n",
      "layer4.2.bn2.running_mean \t torch.Size([512])\n",
      "layer4.2.bn2.running_var \t torch.Size([512])\n",
      "layer4.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer4.2.conv3.weight \t torch.Size([2048, 512, 1, 1])\n",
      "layer4.2.bn3.weight \t torch.Size([2048])\n",
      "layer4.2.bn3.bias \t torch.Size([2048])\n",
      "layer4.2.bn3.running_mean \t torch.Size([2048])\n",
      "layer4.2.bn3.running_var \t torch.Size([2048])\n",
      "layer4.2.bn3.num_batches_tracked \t torch.Size([])\n",
      "fc.weight \t torch.Size([120, 2048])\n",
      "fc.bias \t torch.Size([120])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'initial_lr': 0.001, 'params': [2009406371784, 2009406372024]}]\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validate Model\n",
    "\n",
    "def train_model(loader, model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    \n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    history  = {\n",
    "        'train_acc': [],\n",
    "        'train_los': [],\n",
    "        'valid_acc': [],\n",
    "        'valid_los': []\n",
    "    }\n",
    "    \n",
    "    dataset_sizes = {\n",
    "        'train': len(loader['train'].dataset),\n",
    "        'valid': len(loader['valid'].dataset)\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        for phase in ['train', 'valid']:\n",
    "            \n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            \n",
    "            for inputs, labels, iids in loader[phase]:\n",
    "                if use_gpu:\n",
    "                    inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs.data, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # statistic\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            \n",
    "            data_size = dataset_sizes[phase]\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_epoch_loss = running_loss / data_size\n",
    "                train_epoch_acc  = running_corrects / data_size\n",
    "            else:\n",
    "                valid_epoch_loss = running_loss / data_size\n",
    "                valid_epoch_acc  = running_corrects / data_size\n",
    "\n",
    "            if phase == 'valid' and valid_epoch_acc > best_acc:\n",
    "                best_acc = valid_epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        history['train_acc'].append(train_epoch_acc.item())\n",
    "        history['train_los'].append(train_epoch_loss)\n",
    "        history['valid_acc'].append(valid_epoch_acc.item())\n",
    "        history['valid_los'].append(valid_epoch_loss)\n",
    "        \n",
    "        print('Epoch [{:3d}/{:3d}] train loss: {:.4f} acc: {:.4f}' \n",
    "              '\\n                valid loss: {:.4f} acc: {:.4f}'.format(\n",
    "                  epoch, num_epochs - 1,\n",
    "                  train_epoch_loss, train_epoch_acc, \n",
    "                  valid_epoch_loss, valid_epoch_acc))\n",
    "\n",
    "    print('\\nBest val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_acc, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [  0/  2] train loss: 0.2223 acc: 0.3514\n",
      "                valid loss: 0.1403 acc: 0.6601\n",
      "Epoch [  1/  2] train loss: 0.1203 acc: 0.6845\n",
      "                valid loss: 0.0842 acc: 0.7423\n",
      "Epoch [  2/  2] train loss: 0.0833 acc: 0.7640\n",
      "                valid loss: 0.0634 acc: 0.7932\n",
      "\n",
      "Best val Acc: 0.793154\n",
      "Training time:   8.766013 minutes\n"
     ]
    }
   ],
   "source": [
    "NumEpochs = 3\n",
    "start_time = time.time()\n",
    "\n",
    "best_model, best_acc, history = train_model(\n",
    "    dataLoader, model, criterion, optimizer, exp_lr_scheduler, NumEpochs\n",
    ")\n",
    "    \n",
    "print('Training time: {:10f} minutes'.format((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'train_acc': [0.3513513505458832, 0.6844808459281921, 0.7639721035957336],\n",
      "  'train_los': [0.2222771003279255, 0.12028317928489152, 0.0833145446863066],\n",
      "  'valid_acc': [0.6601467132568359, 0.7422983050346375, 0.7931540608406067],\n",
      "  'valid_los': [0.14028082931537209, 0.08422903291753568, 0.06343682956287505]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(history, indent=2)\n",
    "\n",
    "train_acc = history['train_acc']\n",
    "train_los = history['train_los']\n",
    "valid_acc = history['valid_acc']\n",
    "valid_los = history['valid_los']\n",
    "\n",
    "# for i in range(len(history)):\n",
    "#     train_acc.append(history[i]['train_acc'])\n",
    "#     train_los.append(history[i]['train_loss'])\n",
    "#     valid_acc.append(history[i]['valid_acc'])\n",
    "#     valid_los.append(history[i]['valid_loss'])\n",
    "        \n",
    "def plot_lines(y1, y2, label=None, ax=None):\n",
    "    epochs = len(y1)\n",
    "    x = np.linspace(0, epochs, num=epochs)\n",
    "    ax.plot(x, y1, label=\"acc\")\n",
    "    ax.plot(x, y2, label=\"loss\")\n",
    "    # ax.set_title('training v.s. validate')\n",
    "    ax.set_title(label)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Percentage')\n",
    "    \n",
    "    \n",
    "# 1. Plot in same line, this would work\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "plot_lines(train_acc, train_los, 'training', ax1)\n",
    "plt.legend()\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "plot_lines(valid_acc, valid_los, 'validate', ax2)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "currDT = datetime.now()\n",
    "currStr = currDT.strftime(\"%Y%m%d-%H%M\")\n",
    "\n",
    "acc_str = int(best_acc * 100)\n",
    "fname_best_model = 'resnet50_{}_acc{}.pth'.format(currStr, acc_str)\n",
    "\n",
    "best_model_wts = best_model.state_dict()\n",
    "\n",
    "OutPath = Params['OutPath']\n",
    "best_model_out = join(OutPath, fname_best_model)\n",
    "\n",
    "torch.save(best_model_wts, best_model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"DataPath\": \"D:\\\\GitWork\\\\dog_breed\\\\data\",\n",
      "    \"OutPath\": \"D:\\\\GitWork\\\\dog_breed\\\\output\",\n",
      "    \"ProcPath\": \"D:\\\\GitWork\\\\dog_breed\\\\processed\",\n",
      "    \"PreTrainPath\": \"D:\\\\GitWork\\\\dog_breed\\\\pretrained\",\n",
      "    \"PreTrainFile\": \"resnet50_20200923-1604_acc79.pth\",\n",
      "    \"LoadPreModel\": false,\n",
      "    \"TestPath\": \"D:\\\\GitWork\\\\dog_breed\\\\data\\\\test\",\n",
      "    \"TrainPath\": \"D:\\\\GitWork\\\\dog_breed\\\\data\\\\train\",\n",
      "    \"CsvLabel\": \"labels.csv\",\n",
      "    \"BatchSize\": 16,\n",
      "    \"FracForTrain\": 0.8\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Save parameters to json file\n",
    "\n",
    "fname = 'Params_{}.json'.format(currStr)\n",
    "f_abspath = join(OutPath, fname)\n",
    "\n",
    "json_str = json.dumps(Params, indent=4)\n",
    "print(json_str)\n",
    "\n",
    "with open(f_abspath, 'w') as fout:\n",
    "    fout.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"scottish_deerhound\": 0,\n",
      "    \"maltese_dog\": 1,\n",
      "    \"afghan_hound\": 2,\n",
      "    \"entlebucher\": 3,\n",
      "    \"bernese_mountain_dog\": 4,\n",
      "    \"shih-tzu\": 5,\n",
      "    \"great_pyrenees\": 6,\n",
      "    \"pomeranian\": 7,\n",
      "    \"basenji\": 8,\n",
      "    \"samoyed\": 9,\n",
      "    \"airedale\": 10,\n",
      "    \"tibetan_terrier\": 11,\n",
      "    \"leonberg\": 12,\n",
      "    \"cairn\": 13,\n",
      "    \"beagle\": 14,\n",
      "    \"japanese_spaniel\": 15,\n",
      "    \"australian_terrier\": 16,\n",
      "    \"blenheim_spaniel\": 17,\n",
      "    \"miniature_pinscher\": 18,\n",
      "    \"irish_wolfhound\": 19,\n",
      "    \"lakeland_terrier\": 20,\n",
      "    \"saluki\": 21,\n",
      "    \"papillon\": 22,\n",
      "    \"whippet\": 23,\n",
      "    \"siberian_husky\": 24,\n",
      "    \"norwegian_elkhound\": 25,\n",
      "    \"pug\": 26,\n",
      "    \"chow\": 27,\n",
      "    \"italian_greyhound\": 28,\n",
      "    \"pembroke\": 29,\n",
      "    \"ibizan_hound\": 30,\n",
      "    \"border_terrier\": 31,\n",
      "    \"newfoundland\": 32,\n",
      "    \"lhasa\": 33,\n",
      "    \"silky_terrier\": 34,\n",
      "    \"bedlington_terrier\": 35,\n",
      "    \"dandie_dinmont\": 36,\n",
      "    \"irish_setter\": 37,\n",
      "    \"sealyham_terrier\": 38,\n",
      "    \"rhodesian_ridgeback\": 39,\n",
      "    \"old_english_sheepdog\": 40,\n",
      "    \"collie\": 41,\n",
      "    \"boston_bull\": 42,\n",
      "    \"english_foxhound\": 43,\n",
      "    \"bouvier_des_flandres\": 44,\n",
      "    \"african_hunting_dog\": 45,\n",
      "    \"schipperke\": 46,\n",
      "    \"kelpie\": 47,\n",
      "    \"weimaraner\": 48,\n",
      "    \"bloodhound\": 49,\n",
      "    \"bluetick\": 50,\n",
      "    \"saint_bernard\": 51,\n",
      "    \"labrador_retriever\": 52,\n",
      "    \"chesapeake_bay_retriever\": 53,\n",
      "    \"norfolk_terrier\": 54,\n",
      "    \"english_setter\": 55,\n",
      "    \"wire-haired_fox_terrier\": 56,\n",
      "    \"kerry_blue_terrier\": 57,\n",
      "    \"scotch_terrier\": 58,\n",
      "    \"yorkshire_terrier\": 59,\n",
      "    \"groenendael\": 60,\n",
      "    \"greater_swiss_mountain_dog\": 61,\n",
      "    \"irish_terrier\": 62,\n",
      "    \"basset\": 63,\n",
      "    \"keeshond\": 64,\n",
      "    \"west_highland_white_terrier\": 65,\n",
      "    \"gordon_setter\": 66,\n",
      "    \"malamute\": 67,\n",
      "    \"affenpinscher\": 68,\n",
      "    \"toy_poodle\": 69,\n",
      "    \"clumber\": 70,\n",
      "    \"mexican_hairless\": 71,\n",
      "    \"dingo\": 72,\n",
      "    \"standard_poodle\": 73,\n",
      "    \"miniature_poodle\": 74,\n",
      "    \"staffordshire_bullterrier\": 75,\n",
      "    \"welsh_springer_spaniel\": 76,\n",
      "    \"toy_terrier\": 77,\n",
      "    \"sussex_spaniel\": 78,\n",
      "    \"norwich_terrier\": 79,\n",
      "    \"appenzeller\": 80,\n",
      "    \"irish_water_spaniel\": 81,\n",
      "    \"miniature_schnauzer\": 82,\n",
      "    \"black-and-tan_coonhound\": 83,\n",
      "    \"cardigan\": 84,\n",
      "    \"dhole\": 85,\n",
      "    \"shetland_sheepdog\": 86,\n",
      "    \"rottweiler\": 87,\n",
      "    \"english_springer\": 88,\n",
      "    \"great_dane\": 89,\n",
      "    \"german_short-haired_pointer\": 90,\n",
      "    \"boxer\": 91,\n",
      "    \"bull_mastiff\": 92,\n",
      "    \"borzoi\": 93,\n",
      "    \"pekinese\": 94,\n",
      "    \"cocker_spaniel\": 95,\n",
      "    \"american_staffordshire_terrier\": 96,\n",
      "    \"doberman\": 97,\n",
      "    \"brittany_spaniel\": 98,\n",
      "    \"malinois\": 99,\n",
      "    \"standard_schnauzer\": 100,\n",
      "    \"flat-coated_retriever\": 101,\n",
      "    \"redbone\": 102,\n",
      "    \"border_collie\": 103,\n",
      "    \"curly-coated_retriever\": 104,\n",
      "    \"kuvasz\": 105,\n",
      "    \"chihuahua\": 106,\n",
      "    \"soft-coated_wheaten_terrier\": 107,\n",
      "    \"french_bulldog\": 108,\n",
      "    \"vizsla\": 109,\n",
      "    \"tibetan_mastiff\": 110,\n",
      "    \"german_shepherd\": 111,\n",
      "    \"giant_schnauzer\": 112,\n",
      "    \"walker_hound\": 113,\n",
      "    \"otterhound\": 114,\n",
      "    \"golden_retriever\": 115,\n",
      "    \"brabancon_griffon\": 116,\n",
      "    \"komondor\": 117,\n",
      "    \"briard\": 118,\n",
      "    \"eskimo_dog\": 119\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Save breed dict to json file\n",
    "fname = 'BreedDict_{}.json'.format(currStr)\n",
    "f_abspath = join(OutPath, fname)\n",
    "\n",
    "json_str = json.dumps(dict_bid_fw, indent=4)\n",
    "print(json_str)\n",
    "\n",
    "with open(f_abspath, 'w') as fout:\n",
    "    fout.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
